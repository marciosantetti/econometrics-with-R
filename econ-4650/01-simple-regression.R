

#== ECON 4650-001 -- Spring 2021
#== Marcio Santetti

#=============================================================================#
#                          SIMPLE LINEAR REGRESSION                           #
#=============================================================================#


#== IMPORTANT: Before any operations, make sure to set your working directory.
# In other words, you have to tell R in which folder you will save your work, or
# from which folder external data sets will come from. In the lower-right pane, 
# click on 'Files'. Select your desired folder, and click on 'More', then select 
# the option 'Set as Working Directory'.

#==============================================================================#



#== Installing and/or loading required packages:


library(tidyverse)
library(broom)          ## for regression objects manipulation.
library(wooldridge)     ## for Wooldrige's book data sets.


#==============================================================================#



##== In this script, we finally start doing Econometrics in practice. Without further due,
## let us start with a data set from IPUMS (Integrated Public Use Microdata Series), 
## a large population database at the individual level. The data set we will use here
## is from 2018. For more info, see [https://ipums.org].


ipums_data <- read_csv('ipums_data.csv')



glimpse(ipums_data)     ## a quick look at the data.



##== For this lecture, we will use the following variables:

# hh_size: household size (# people);
# hh_income: household total income ($ 10,000);
# cost_housing: reported cost of housing ($ per week);
# n_vehicles: household's # vehicles;
# time_commuting: average time spent commuting per day (in minutes).



## Some scatter plots:


# Commuting time vs. household income:

ipums_data %>% ggplot(mapping = aes(x=hh_income, y=time_commuting)) + geom_point() + theme_bw()


# Commuting time vs. # vehicles:

ipums_data %>% ggplot(mapping = aes(x=n_vehicles, y=time_commuting)) + geom_point() + theme_bw()


# Housing cost vs. household size:

ipums_data %>% ggplot(mapping = aes(x=hh_size, y=cost_housing)) + geom_point() + theme_bw()




##== Simple Regression Model 1:


## Regression models in R are calculated with the 'lm()' function. lm = 'linear model'.

## The syntax is: lm(dependent ~ independent variable, data set).


# Commuting time vs. household income:


simple_reg1 <- lm(time_commuting ~ hh_income, 
                  data = ipums_data)          




summary(simple_reg1)           ## for a comprehensive regression output summary (recommended).

simple_reg1 %>% tidy()         ## a reduced summary tibble.


## Drawing a regression line:

# To draw the regression line generated by OLS, we simply add the 'geom_smooth()' function to a scatter plot.


ipums_data %>% ggplot(mapping = aes(x=hh_income, y=time_commuting)) + geom_point() + theme_bw() +
  geom_smooth(method = "lm", se=FALSE, color='red')




##== Simple Regression Model 2:


# Commuting time vs. # vehicles:


simple_reg2 <- lm(time_commuting ~ n_vehicles, data = ipums_data)

summary(simple_reg2)



# (Students) Draw the regression line:

ipums_data %>% ggplot(mapping = aes(x=n_vehicles, y=time_commuting)) + geom_point(color='red') +
  geom_smooth(method='lm', se=FALSE) + theme_minimal()




##== Simple Regression Model 3:


# Housing cost vs. household size:


simple_reg3 <- lm(cost_housing ~ hh_size, data = ipums_data)

summary(simple_reg3)

simple_reg3 %>% tidy()



# (Students) Draw the regression line:



##====================================================================================##



##== Using data from the 'wooldridge' package (data from Wooldridge's book):


## The 'beauty' data set brings microeconometric data on wages, education, experience, gender, etc.

## Reference: Hamermesh, D.S. and J.E. Biddle (1994), "Beauty and the Labor Market,"
## American Economic Review 84, 1174-1194.


data("beauty")                         ## requiring the data set.


beauty <- as_tibble(beauty)            ## transforming into a 'tibble'.

glimpse(beauty)


##=== Simple Regression Model 4:


## Scatter plot (wages vs. experience):


beauty %>% ggplot(mapping = aes(x=exper, y=wage)) + geom_point(alpha=1/3)   ## Interpretation?


simple_reg4 <- lm(wage ~ exper, data = beauty)

summary(simple_reg4)




##== Manually calculating regression estimates:

## Calculating simple regression coefficients (intercept and slope) is pretty easy. In order
## to give you more intuition, let us do that for this and the next model.


## To compute the slope coefficient, we need the covariance between the dependent (y) and the independent (x)
## variable, as well as the variance of the independent variable (x).


## And for the intercept term, in addition to the slope, we need the sample means of x and y.


## In summary: slope (beta1) = Cov(x,y)/Var(x)
##             intercept (beta0) = y.bar - x.bar*beta1


## We can easily calculate all components using the 'summarize()' function:



beauty %>% summarize(covariance = cov(wage, exper),
                     mean_wage = mean(wage),
                     mean_exper = mean(exper),
                     corr_coef = cor(wage, exper),
                     slope_coef = covariance/var(exper),
                     intercept = mean_wage - slope_coef*mean_exper)   ## Do the results match with the
                                                                      ## ones given by the 'summary()' function?




##=== Simple Regression Model 5:


## Scatter plot (wages vs. education):


beauty %>% ggplot(mapping = aes(x=educ, y=wage)) + geom_point(alpha=1/3)


## Model:


simple_reg5 <- lm(wage ~ educ, data = beauty)

summary(simple_reg5)


## Manual calculation:


beauty %>% summarize(covariance = cov(wage, educ),
                     mean_wage = mean(wage),
                     mean_educ = mean(educ),
                     corr_coef = cor(wage, educ),
                     slope_coef = covariance/var(educ),
                     intercept = mean_wage - slope_coef*mean_educ)    ## Do the results match with the
                                                                      ## ones given by the 'summary()' function?






##==================== Residuals and fitted values:


## It is possible to access relevant elements from regression objects, such as 
## the model's error term and fitted values.



resid <- simple_reg5 %>% resid()


resid %>% plot()     ## a simple plot of the regression's residuals.



## Is the mean of the residual term zero, as we theoretically assume?
## This is pretty easy to assess:


resid %>% mean()    ## Answer?



## What about the relationship between the independent variable (x) and the residual term (u)?


beauty %>% summarize(cov_x_u = cov(resid, educ))   ## Answer? 



## From a regression object, we can also extract its fitted (estimated) values (y.hat):


simple_reg5 %>% fitted() 


## And a plot:


simple_reg5 %>% fitted() %>% plot()



## Finally, y - y.hat should be equal to the residual term, u.


y.hat <- simple_reg5 %>% fitted()


y <- beauty %>% select(wage)


is_it_u <- y - y.hat


is_it_u %>% head()   ## for the first six observations, we use 'head()'. 
                     ## If you want the last six, use 'tail()'.

resid %>% head()     ## Are they the same?


##====================================================================================##


##== (Students) load the 'ceosal1' data set from 'wooldridge.
## Run a simple regression of 'sales' on 'salary'.




##====================================================================================##


#========== Practice:


#== The 'food.csv' data set is available at this week's Canvas page. It contains only
#== two variables: weekly food expenditures (in dollars), and weekly income (in hundreds of dollars).
#== Import this data set into your R environment, and play around with its variables. You
#== can do some basic statistical analysis, and then estimate a regression model, both with
#== the 'lm()' command and with manual calculations. The important thing is to make yourself 
#== comfortable with these concepts and the R practice. And even more importantly: enjoy your
#== practice time!


#=====================================================================================#




